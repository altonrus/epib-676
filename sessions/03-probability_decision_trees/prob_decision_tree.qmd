---
title: "Probability, decision trees"
subtitle: "EPIB  676 session 3, McGill University"
author: "Alton Russell"
date: "5 September 2024"
format: revealjs
editor: visual
---

## Today

```{r}
library(flextable)
```

-   **Key probability concepts**

-   Decision trees

## What is probability?

## What is probability?

Relative frequency: long run average over many repetitions

<br>

<br>

Measure of belief: how likely is the outcome?

## What is probability?

Relative frequency: long run average over many repetitions

*How probabilities are estimated in frequentist statistics*

<br>

Measure of belief: how likely is the outcome?

*How probabilities are used in decision analysis (and, roughly, in Bayesian statistics)*

## Sample space and events

**Sample space (**$\Omega$**):** set of all possible outcomes

**Event:** one outcome or set of outcomes (subset of sample space)

<br>

![](figs/sample_space.svg)

## Axioms of probability 1 & 2

1.  Probability of an event is between 0 (can't happen) and 1 (must happen)

$$ 0 \leq P(e) \leq 1 $$

2.  Some event in the sample space must occur

$$
P(\Omega) = 1
$$

## Axiom 3: Mutually exclusive

For **mutually exclusive** (or 'disjoint') events $e_1$, $e_2$, ... $e_I$:

$$
P(\cup_{i=1}^{I} e_i) = \sum_{i=1}^{I} P(e_i)
$$

The probability [**any**]{.underline} mutually exclusive event occurs it the sum of the events' individual probabilities

![](figs/mutually_exclusive.svg)

## Conditional probability

-   Prob. of $A$ occurring [**conditioned on**]{.underline} $B$ having occurred (probability $A$ [**given**]{.underline} $B$):

$$
P(A \mid B)=\frac{P(AB)}{P(B)}
$$

-   Can condition on multiple events:

$$
P(A \mid BCDE) = \frac{P(ABCDE)}{P(BCDE)}
$$

## Conditional probability visualized

'Rescaling' the sample space to account for B occurring

![<https://www.geeksforgeeks.org/conditional-probability/>](figs/conditional_prob.png)

## Conditional probability for mutually exclusive events?

![](figs/mutually_exclusive.svg){width="188"}

What is $P(A \mid B)$?

## Conditional probability for mutually exclusive events?

![](figs/mutually_exclusive.svg){width="188"}

What is $P(A \mid B)$?

For **mutually exclusive** events $A, B$:

$$
P(A \mid B) = 0\\
P(B \mid A) = 0
$$

## Independent events

If $A \perp \!\!\! \perp B$, A occurring doesn't effect B's probability and B occurring doesn't effect A's probability

$$
P(A) = P(A \mid B) = P(A \mid B')\\
P(B) = P(B \mid A) = P(B \mid A')\\
P(AB) = P(A)P(B)
$$

![](figs/independent_events.svg)

## Conditional independence

A and B are independent conditioned on C iff

$$
P(A \mid BC) = P(A \mid C)\\
P(B \mid AC) = P(B \mid C)\\
P(AB \mid C) = P( A \mid C)P( B \mid C)
$$

Example: Are human height and vocabulary independent?

## Conditional independence

A and B are independent conditioned on C iff

$$
P(A \mid BC) = P(A \mid C)\\
P(B \mid AC) = P(B \mid C)\\
P(AB \mid C) = P( A \mid C)P( B \mid C)
$$

Ex: Are Canadians' height and vocabulary independent?

> No, shorter humans are more likely to be children who know fewer words. But height and vocabulary are likely independent **conditioned on** a person being \>20 years old

## Bayes theorem

$$
P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)}
$$

<br>

"Post-test probability of disease" is a classic example:

$$
P(\text{disease+} | \text{test+}) = \frac{P(\text{test+} \mid \text{disease+})P(\text{disease+})}{P(\text{test+})}
$$

## Contingency table for binary test

This 2x2 table (with marginal totals) describes results from screening 10,000 people for a disease

<br>

*Copy and paste this code and run on your computer*

```{r}
#| echo: true
disease = c(rep(0,9500), rep(1,500))
test_pos = c(rep(1,285), rep(0, 10000 - 285 - 163), rep(1, 163))
table(disease, test_pos)|> addmargins()
```

## Prevalence, sensitivity, specificity

::: columns
::: {.column width="65%"}
```{r}
#| echo: false
disease = c(rep(0,9500), rep(1,500))
test_pos = c(rep(1,285), rep(0, 10000 - 285 - 163), rep(1, 163))
table(disease, test_pos)  |> addmargins()
```

<br>

From the data, estimate:

-   Prevalence $P(disease+)$

-   Sensitivity $P(test + \mid disease+)$

-   Specificity $P(test - \mid disease-)$
:::

::: {.column width="35%"}
$$
P(A \mid B)=\frac{P(AB)}{P(B)}
$$
:::
:::

## Prevalence, sensitivity, specificity

```{r}
#| echo: false
disease = c(rep(0,9500), rep(1,500))
test_pos = c(rep(1,285), rep(0, 10000 - 285 - 463), rep(1, 463))
table(disease, test_pos)  |> addmargins()
```

Prevalence $P(disease+)$: `r sum(disease)/length(disease)`

`sum(disease)/length(disease)`

Sensitivity $P(test + \mid disease+)$: `r sum(test_pos*disease)/sum(disease)`

`sum(test_pos*disease)/sum(disease)`

Specificity $P(test - \mid disease-)$: `r sum((1-test_pos)*(1-disease))/sum(1-disease)`

`sum((1-test_pos)*(1-disease))/sum(1-disease)`

## Pre- and post-test probability

```{r}
#| echo: false
disease = c(rep(0,9500), rep(1,500))
test_pos = c(rep(1,285), rep(0, 10000 - 285 - 163), rep(1, 163))
table(disease, test_pos)  |> addmargins()
```

We test a new person from the same population. What is their:

-   Pre-test prob. of having the disease?

-   Post-test prob. if test is positive (positive predictive value)?

-   Post-test prob. if test is negative (one minus negative predictive value)?

## Pre- and post-test probability

```{r}
#| echo: false
disease = c(rep(0,9500), rep(1,500))
test_pos = c(rep(1,285), rep(0, 10000 - 285 - 163), rep(1, 163))
table(disease, test_pos)  |> addmargins()
```

<br>

Pre-test probability = prevalence = $P(disease+)=$ `r sum(disease)/length(disease)`

PPV = $P(disease+ \mid test+)$ = `r sum(disease*test_pos)/sum(test_pos)` `sum(disease*test_pos)/sum(test_pos)`

1 - NPV = $P(disease+ \mid test-)$= `r sum(disease*(1-test_pos))/sum(1-test_pos)`

`sum(disease*(1-test_pos))/sum(1-test_pos)`

## Bayesian updating

What's my distribution of uncertainy in parameter $\theta$, based on prior distribution $P(\theta)$ and data $x$?

$$
P( \theta \mid x) = \frac{P(x \mid \theta)  P(\theta)}{P(x)}, \text{  }
\text{Posterior} = \frac{\text{Likelihood} \times Prior}{Evidence}
$$

**Posterior** $P(\theta \mid x)$: updated belief after seeing data

**Prior** $P(\theta)$: belief before receiving data

**Likelihood** $P(x \mid \theta)$: probability of the data given prior

**Probability of the data/evidence** $P(x)$: think of it as a normalization factor

## Post-test probability as Bayesian updating

$$
P(\text{disease+} | \text{test+}) = \frac{P(\text{test+} \mid \text{disease+}) \times P(\text{disease+})}{P(\text{test+})}
$$ What is my prior? Likelihood? Posterior?

## Expected value of a random variable

Long run average value from repeated observations

<br>

::: columns
::: {.column width="50%"}
**Discrete** variable $x$ with $I$ possible values $x_i$

$$
E[x] = \sum_{i=1}^I x_iP(x_i)
$$
:::

::: {.column width="50%"}
**Continuous** variable $x$

<br>

$$
E[x] = \int_{-\infty}^{\infty} x f(x)dx
$$
:::
:::

## Belief about a probability

Binary random events follow a **Bernoulli** distribution:

$$
x \sim Bernoulli(p) =
\begin{cases}
1 & \text{with probability } p\\
0 & \text{with probability } 1-p
\end{cases}
$$

The **beta distribution** models our belief (uncertainty) about probability $p$

$$
p \sim Beta(\alpha, \beta) = \frac{1}{B(\alpha, \beta)} p^{\alpha - 1} (1-p)^{\beta - 1}
$$

## Beta distribution properties

::: columns
::: {.column width="45%"}
-   Range $[0,1]$

-   Shape parameter $\alpha > 0$

-   Scale parameter $\beta > 0$

-   Mean $E[p] = \frac{\alpha}{\alpha+\beta}$
:::

::: {.column width="55%"}
![https://en.wikipedia.org/wiki/Beta_distribution](figs/PDF_of_the_Beta_distribution.gif){fig-align="center" width="400"}
:::
:::

## Beta distribution and binomial experiments

-   In binomial experiments (repeated independent Bernoulli trials), we observe n realizations of a bernoulli variable (e.g., \[0,0,1,0,1,1,...\])

    -   Binomial distribution describes how many 1's we expect based on probability $p$ and number of trials $n$

-   For a binomial trial with $E$ events (1's) and $N$ non-events (0's):

    -   $p \sim Beta(\alpha=E,\beta=N)$ describes our uncertainty in $p$

## Bayesian updating with beta

In a small drug study, 1 in 3 patients had a side-effect. If we have no other information, our uncertainty in the probability of side-effects can be represented as:

$$
p \sim Beta(\alpha = 1, \beta = 2) \quad E[p] = 0.333
$$

```{r}
#| echo: true
p = seq(0, 1, length=1000)
plot(p, dbeta(p, 1, 2), type='l')
```

## Bayesian updating with beta

We administer the drug to 40 new patients, and 4 of them have a side effect. What distribution describes our uncertainty in the probability of side effect now?

## Bayesian updating with beta

We administer to 40 new patients, and 4 of them have a side effect. What distribution describes my uncertainty in the probability of side effect now?

$$
p \sim Beta(\alpha = 5, \beta = 38) \quad E[p] = 0.116
$$

```{r}
#| echo: true
plot(p, dbeta(p, 5, 38), type='l')
```

## Bayesian updating with beta

In 10,000 total patients, 1,230 have a side effect:

$$
p \sim Beta(\alpha = 1230, \beta = 8770) \quad E[p] = 0.1230
$$

```{r}
#| echo: true
plot(p, dbeta(p, 1230, 10000 - 1230), type='l')
```

## Beta distribution summary

-   Describes belief of values an uncertain probability may take

-   Parameters $\alpha$ and $\beta$ map to event and non-event counts from a binomial experiment

-   As we collect data:

    -   $\alpha$ and $\beta$ grow

    -   Our certainty increases

    -   The distribution gets more concentrated

## Beta updating animation^1^

::: columns
::: {.column width="25%"}
![](figs/priors.webp){fig-align="left"}

Priors ↑

Posteriors ↓

![](figs/posteriors.webp){fig-align="left"}
:::

::: {.column width="40%"}
![](figs/updating_priors.webp)
:::

::: {.column width="35%"}
**Priors**:

Beta(5,5)

Beta(75,75)

**Data**:

145 successes

208 trials

**Posteriors**:

Beta(150, 68)

Beta(220, 138)
:::
:::

^1^ <https://www.r-bloggers.com/2020/04/an-animated-example-of-bayesian-updating/>

## Today

-   Key probability concepts

-   **Decision trees**

## Decision tree convention

-   Nodes connected by branches

-   "Grows" from left to right

-   Starts with 'root node' (usually a 'decision node') ends in 'leaf nodes ('terminal nodes')

-   "Payoffs" received at terminal nodes (sometimes, payoffs received along the way)

## Decision tree symbols

::: columns
::: {.column width="40%"}
![](figs/decision_tree_nodes.svg)
:::

::: {.column width="50%"}
-   Things we control are decisions (actions)

-   Random events are chances (reactions)
:::
:::

## Parameters and tree structure

::: columns
::: {.column width="60%"}
**Alternatives**

**Random events**

**Probabilities**

**Costs**

**Health effects (e.g., QALYs)**
:::

::: {.column width="40%"}
![](figs/decision_tree_nodes.svg)
:::
:::

## Parameters and tree structure

**Alternatives (actions)** → branches from a decision node **▢**

**Random events (reactions)** → branches from a chance node ◯

**Probabilities** → assigned to branches from a chance node

**Costs** → Payoffs (at terminal nodes △ or along the way)

**Health effects** → Payoffs (at terminal nodes △ or along the way)

## Parameters conditional on path

::: columns
::: {.column width="60%"}
![](figs/tree_example.svg)
:::

::: {.column width="40%"}
-   Probabilities from chance nodes must sum to 1

-   Probabilities are conditional on earlier events

-   Payoffs at terminal nodes are path-specific
:::
:::

## Preventing stent for vascular disease[^1]

[^1]: Source: [rdecision package vignette](https://cran.rstudio.com/web/packages/rdecision/vignettes/DT00-DecisionTreeTutorial.html)

**Decision**: enroll patient in diet or exercise program?

**Chances**: will they need a stent?

**Outcome**: cost, QALYs lost related to receiving a stent (+ program cost)

**Time horizon:** one year

## Stent prevention tree

```{r}
#Use install.packages("XXXX") if you don't have any of these installed
library(rdecision) #decision trees
library(flextable) #Formatting tables to display (https://davidgohel.github.io/flextable/reference/index.html)
library(ggplot2) #Plotting
library(readxl) #for read_excel()
library(dplyr) # I use mutate at one point
theme_set(theme_bw()) #Makes ggplots look better

# Decision problem: Should we use diet or excercise to reduce chance of needing a stent in a high-risk population?

# Parameters
c_diet <- 50 #cost of diet
c_excercise <- 750 #cost of exercise
c_stent <- 5000 #cost of a stent
u_stent <- 0.75 #utility of getting a stent (relative to 1.0)
p_stent_diet <- (68 - 12)/68 #probability needing stent if we diet
p_stent_exercise <- (58 - 18)/58 #probability of needing a stent if we exercise

#Build model using rdecision package

#Create decision and chance nodes
decision_node <- DecisionNode$new("Diet or excercise")
chance_node_diet <- ChanceNode$new("Stent?")
chance_node_exercise <- ChanceNode$new("Stent?")

#Create leaf nodes
leaf_node_diet_no_stent <- LeafNode$new("No stent")
leaf_node_diet_stent <- LeafNode$new("Stent", utility = u_stent)
leaf_node_exercise_no_stent <- LeafNode$new("No stent")
leaf_node_exercise_stent <- LeafNode$new("Stent", utility = u_stent)

#Create 'actions', paths from your decision node(s)
action_diet <- Action$new(
  decision_node, chance_node_diet, cost = c_diet, label = "Diet"
)
action_exercise <- Action$new(
  decision_node, chance_node_exercise, cost = c_excercise, label = "Exercise"
)

#Create 'reactions', paths from your chance node(s)
reaction_diet_success <- Reaction$new(
  chance_node_diet, leaf_node_diet_no_stent, 
  p = 1-p_stent_diet, cost = 0.0, label = "Did not need stent")

reaction_diet_failure <- Reaction$new(
  chance_node_diet, leaf_node_diet_stent, 
  p = p_stent_diet, cost = c_stent, label = "Needed stent")

reaction_exercise_success <- Reaction$new(
  chance_node_exercise, leaf_node_exercise_no_stent, 
  p = 1-p_stent_exercise, cost = 0.0, label = "Did not need stent")

reaction_exercise_failure <- Reaction$new(
  chance_node_exercise, leaf_node_exercise_stent, 
  p = p_stent_exercise, cost = 5000.0, label = "Needed stent")

#Create, draw, and evaluate the tree
DT1 <- DecisionTree$new(
  V = list(decision_node, #verticies (nodes)
           chance_node_diet, 
           chance_node_exercise, 
           leaf_node_diet_no_stent, 
           leaf_node_diet_stent, 
           leaf_node_exercise_no_stent, 
           leaf_node_exercise_stent),
  E = list(action_diet, #edges (paths between nodes)
           action_exercise,
           reaction_diet_success,
           reaction_diet_failure,
           reaction_exercise_success,
           reaction_exercise_failure)
)

DT1$draw() #Plot it
```

## Stent prevention data

The healthcare provider cost of the stent is **5000 GBP**; the cost of providing lifestyle advice, an appointment with a dietician, is **50 GBP** and the cost of the exercise programme, is **750 GBP**.

In a small trial of the "diet" programme, **12 out of 68** patients (17.6%) avoided having a procedure. in a small trial of the "exercise" programme **18 out of 58** patients (31.0%) avoided the procedure (assume the baseline characteristics in the two trials were comparable). A stent leads to one year with a health state utility of **0.75**.

## Stent prevention parameters

```{r}
#| echo: true
# Parameters
c_diet <- 50 #cost of diet intervention
c_excercise <- 750 #cost of exercise
c_stent <- 5000 #cost of a stent
qaly_stent <- 0.75 #QALYs experienced with stent
qaly_no_stent <- 1 #QALYs experienced without stent
p_stent_diet <- (68 - 12)/68 #prob. needing stent if diet
p_stent_exercise <- (58 - 18)/58 #prob. needing a stent if exercise
```

## Evaluating (rolling back) a tree

-   For each chance node from **right to left** (going backwards), calculate the expected value of each outcome

    -   Cost, health outcome, etc.

-   Leaves us with expect value of each choice at the root decision node

::: {.column width="50%"}
$$
E[x] = \sum_{i=1}^I x_iP(x_i)
$$
:::

## Roll back: cost minimization example

![](figs/rollback_tree_1.svg)

## Roll back: cost minimization example

![](figs/rollback_tree_2.svg)

## Roll back: cost minimization example

![](figs/rollback_tree_3.png)

## Stent prevention tree

```{r}
DT1$draw() #Plot it
```

## Roll back stent prevention

```{r}
#| echo: true
# Parameters
c_diet <- 50 #cost of diet intervention
c_excercise <- 750 #cost of exercise
c_stent <- 5000 #cost of a stent
qaly_stent <- 0.75 #QALYs experienced with stent
qaly_no_stent <- 1 #QALYs experienced withou stent
p_stent_diet <- (68 - 12)/68 #prob. needing stent if diet
p_stent_exercise <- (58 - 18)/58 #prob. needing a stent if exercise

exp_tot_cost_diet = NA
exp_tot_cost_excercise = NA
exp_qaly_diet = NA
exp_qaly_excercise = NA
ICER = NA
```

## Roll back stent prevention

```{r}
#| echo: true
exp_tot_cost_diet = c_diet + p_stent_diet*c_stent
exp_tot_cost_diet
exp_tot_cost_excercise = c_excercise + p_stent_exercise*c_stent
exp_tot_cost_excercise
exp_qaly_diet = qaly_stent*p_stent_diet + qaly_no_stent*(1-p_stent_diet)
exp_qaly_diet
exp_qaly_excercise = qaly_stent*p_stent_exercise + qaly_no_stent*(1-p_stent_exercise)
exp_qaly_excercise
ICER_excer_vs_diet = (exp_tot_cost_excercise - exp_tot_cost_diet)/(exp_qaly_excercise - exp_qaly_diet)
ICER_excer_vs_diet #GBP per QALY
```

## Rollback with sequential decisions

::: columns
::: {.column width="65%"}
![](figs/sequential_tree.jpg){width="600"}

Source: [Suner et. al. 2012](https://doi-org.proxy3.library.mcgill.ca/10.1016/j.artmed.2012.05.003)
:::

::: {.column width="35%"}
For 'downstream' decision nodes:

-   Roll back to each decision node

-   Select the preferred branch (based on framework)

-   Continue to root decision node
:::
:::

## Strengths of decision trees

-   Intuitive to build and communicate

-   Easy to quickly compute

-   Easy to combined with other modeling methods

## Limitations of decision trees

-   Discrete possibility space

-   No time dimension

-   Not great for chronic conditions or recurring risks

## Recap

-   Key probability building blocks:

    -   Conditional probability

    -   Bayesian updating

    -   Beta distribution for uncertain probabilities

-   Decision trees are simple, flexible, useful decision models

-   "Roll back" the decision tree to calculate the expected value of each outcome under each alternative

## Logistics

-   Complete readings before Thursday's class

-   Assignment 1 due Friday, September 12

-   Office hours 1:30-2:30pm in Room #1122 Tuesdays and Thursdays
